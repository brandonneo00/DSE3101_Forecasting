{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading pop_mvmd.csv: [Errno 2] No such file or directory: 'pop_mvmd.csv'\n",
      "Error reading propi.csv: [Errno 2] No such file or directory: 'propi.csv'\n",
      "Error reading ptax.csv: [Errno 2] No such file or directory: 'ptax.csv'\n",
      "Error reading ratesav.csv: [Errno 2] No such file or directory: 'ratesav.csv'\n",
      "Error reading rcon.csv: [Errno 2] No such file or directory: 'rcon.csv'\n",
      "Error reading rconhh_09Q3.csv: [Errno 2] No such file or directory: 'rconhh_09Q3.csv'\n",
      "Error reading rcons.csv: [Errno 2] No such file or directory: 'rcons.csv'\n",
      "Error reading rconsnp_09Q3.csv: [Errno 2] No such file or directory: 'rconsnp_09Q3.csv'\n",
      "Error reading rex.csv: [Errno 2] No such file or directory: 'rex.csv'\n",
      "Error reading rg.csv: [Errno 2] No such file or directory: 'rg.csv'\n",
      "Error reading rgf.csv: [Errno 2] No such file or directory: 'rgf.csv'\n",
      "Error reading rgsl.csv: [Errno 2] No such file or directory: 'rgsl.csv'\n",
      "Error reading rimp.csv: [Errno 2] No such file or directory: 'rimp.csv'\n",
      "Error reading rinvbf.csv: [Errno 2] No such file or directory: 'rinvbf.csv'\n",
      "Error reading rinvrsid.csv: [Errno 2] No such file or directory: 'rinvrsid.csv'\n",
      "Error reading rnx.csv: [Errno 2] No such file or directory: 'rnx.csv'\n",
      "Error reading ruc_md.csv: [Errno 2] No such file or directory: 'ruc_md.csv'\n",
      "Error reading wsd.csv: [Errno 2] No such file or directory: 'wsd.csv'\n",
      "Error reading cpi_md94Q3.csv: [Errno 2] No such file or directory: 'cpi_md94Q3.csv'\n",
      "Error reading gdp.csv: [Errno 2] No such file or directory: 'gdp.csv'\n",
      "Error reading hstarts_mvmd.csv: [Errno 2] No such file or directory: 'hstarts_mvmd.csv'\n",
      "Error reading ipt_mvmd.csv: [Errno 2] No such file or directory: 'ipt_mvmd.csv'\n",
      "Error reading lfc_mvmd.csv: [Errno 2] No such file or directory: 'lfc_mvmd.csv'\n",
      "Error reading lfpart_mvmd.csv: [Errno 2] No such file or directory: 'lfpart_mvmd.csv'\n",
      "Error reading m1_md.csv: [Errno 2] No such file or directory: 'm1_md.csv'\n",
      "Error reading oli.csv: [Errno 2] No such file or directory: 'oli.csv'\n",
      "Error reading oph.csv: [Errno 2] No such file or directory: 'oph.csv'\n",
      "Error reading oph_98Q4.csv: [Errno 2] No such file or directory: 'oph_98Q4.csv'\n",
      "Error reading p.csv: [Errno 2] No such file or directory: 'p.csv'\n",
      "Error reading pinti.csv: [Errno 2] No such file or directory: 'pinti.csv'\n",
      "Error reading pintpaid.csv: [Errno 2] No such file or directory: 'pintpaid.csv'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rinvchiQvQd.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m rinvchi \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrinvchiQvQd.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m monthly_dataframes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataframes:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Check if the number of rows in the DataFrame is greater than 308\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WAY4SGP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\WAY4SGP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\WAY4SGP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\WAY4SGP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rinvchiQvQd.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Read data\n",
    "file_paths = [\n",
    "    r\"pop_mvmd.csv\",\n",
    "    r\"propi.csv\",\n",
    "    r\"ptax.csv\",\n",
    "    r\"ratesav.csv\",\n",
    "    r\"rcon.csv\",\n",
    "    r\"rconhh_09Q3.csv\",\n",
    "    r\"rcons.csv\",\n",
    "    r\"rconsnp_09Q3.csv\",\n",
    "    r\"rex.csv\",\n",
    "    r\"rg.csv\",\n",
    "    r\"rgf.csv\",\n",
    "    r\"rgsl.csv\",\n",
    "    r\"rimp.csv\",\n",
    "    r\"rinvbf.csv\",\n",
    "    \n",
    "    r\"rinvrsid.csv\",\n",
    "    r\"rnx.csv\",\n",
    "    r\"ruc_md.csv\",\n",
    "    r\"wsd.csv\",\n",
    "    r\"cpi_md94Q3.csv\",\n",
    "    r\"gdp.csv\",\n",
    "    r\"hstarts_mvmd.csv\",\n",
    "    r\"ipt_mvmd.csv\",\n",
    "    r\"lfc_mvmd.csv\",\n",
    "    r\"lfpart_mvmd.csv\",\n",
    "    r\"m1_md.csv\",\n",
    "    r\"oli.csv\",\n",
    "    r\"oph.csv\",\n",
    "    r\"oph_98Q4.csv\",\n",
    "    r\"p.csv\",\n",
    "    r\"pinti.csv\",\n",
    "    r\"pintpaid.csv\"\n",
    "]\n",
    "dataframes = [\"rinvchi\"]\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        # Extracting filename without extension\n",
    "        file_name = file_path.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        # Reading CSV into DataFrame with variable name same as filename\n",
    "        globals()[file_name] = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        \n",
    "        # Printing name and dimensions\n",
    "        #print(\"Name:\", file_name)\n",
    "        #print(\"Dimensions:\", globals()[file_name].shape)\n",
    "        dataframes.append(file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "rinvchi = pd.read_excel(\"rinvchiQvQd.xlsx\")\n",
    "\n",
    "monthly_dataframes = []\n",
    "\n",
    "for file_name in dataframes:\n",
    "    # Check if the number of rows in the DataFrame is greater than 308\n",
    "    if len(globals()[file_name]) > 308:\n",
    "        # Append the file name to the list\n",
    "        monthly_dataframes.append(file_name)\n",
    "def convert (df):\n",
    "    quarterly_data = df[df['DATE'].str.endswith(('03', '06', '09', '12'))].reset_index(drop=True)\n",
    "\n",
    "    #print(quarterly_data.shape)\n",
    "    return quarterly_data\n",
    "\n",
    "for file_name in monthly_dataframes:\n",
    "    globals()[file_name] = convert(globals()[file_name])\n",
    "\n",
    "ipt_mvmd = ipt_mvmd.iloc[-308:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_month_code(lookup):\n",
    "    suffix = lookup[-2:]\n",
    "    month_mapping = {\n",
    "        \"Q1\": \"M3\",\n",
    "        \"Q2\": \"M6\",\n",
    "        \"Q3\": \"M9\",\n",
    "        \"Q4\": \"M12\"\n",
    "    }\n",
    "    if suffix in month_mapping:\n",
    "        return lookup[:2] + month_mapping[suffix]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def data_extract(lookup):\n",
    "    result_df = pd.DataFrame() \n",
    "    gdp_columns = [col for col in gdp if col.endswith(lookup)]\n",
    "    if gdp_columns:\n",
    "        # Extract the column along with its header\n",
    "        result_df = pd.concat([result_df, gdp[\"DATE\"]], axis=1)\n",
    "        #result_df = pd.concat([result_df, gdp[gdp_columns[0]]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #other predictor\n",
    "    for name in dataframes: \n",
    "        df= globals()[name]  \n",
    "        df_q = [col for col in df if col.endswith(lookup)]\n",
    "        if df_q:\n",
    "            result_df = pd.concat([result_df, df[df_q[0]]], axis=1)\n",
    "        else:\n",
    "            mlook = get_month_code(lookup)\n",
    "            df_q = [col for col in df if col.endswith(mlook)]\n",
    "            if df_q:\n",
    "             result_df = pd.concat([result_df, df[df_q[0]]], axis=1)\n",
    "    return result_df\n",
    "\n",
    "#Add lags\n",
    "#use lagged observations (e.g. t-x to t-12) as input variables to forecast the current time step (t) , where x is the number of steps ahead\n",
    "def series_to_supervised(data, col_name, n_in=1, steps = 1):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Sequence of observations as a list, NumPy array, or pandas DataFrame/Series.\n",
    "    Col_name: name of column to transform\n",
    "    n_in: Number of lag observations as input (X).\n",
    "    n_out: Number of observations as output (y).\n",
    "    dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "    Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1\n",
    "    if isinstance(data, list) or isinstance(data, np.ndarray):\n",
    "        n_vars = 1\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        n_vars = data.shape[1]\n",
    "    elif isinstance(data, pd.Series):\n",
    "        n_vars = 1\n",
    "        data = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data type. Please provide data as a list, NumPy array, or pandas DataFrame/Series.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, steps-1,-1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(f'{col_name}(t-{i})')]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    #agg.dropna(subset=[agg.columns[-1]], inplace=True)\n",
    "\n",
    "\n",
    "    return agg\n",
    "\n",
    "def add_na(data, name, n):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Assuming you want to operate on the first column of the DataFrame\n",
    "        yy = data.iloc[:, 0].dropna()\n",
    "    elif isinstance(data, pd.Series):\n",
    "        yy = data.dropna()\n",
    "    else:\n",
    "        raise ValueError(\"Input data must be a DataFrame or Series\")\n",
    "\n",
    "    # Reset the index of yy to ensure unique index values\n",
    "    yy = yy.reset_index(drop=True)\n",
    "\n",
    "    # Create a Series of NaN values with the same length as yy\n",
    "    xx = pd.Series([np.nan] * n, index=range(len(yy)+1, len(yy) + n+1))\n",
    "    # Set the name of the xx series to be the same as the original column name\n",
    "    xx.name = name\n",
    "    return pd.concat([yy, xx])\n",
    "\n",
    "def random_forest (x1, x2,horizon,feature_importance, feature_name, n_tree = 50):\n",
    "    lookup = x1[-2:] + x2\n",
    " \n",
    "\n",
    "    df = data_extract(lookup)\n",
    "    df = df[df.loc[:, df.columns.str.startswith('ROUTPUT')].notna().any(axis=1)]\n",
    "\n",
    "    X_no_lag = df\n",
    "    X_no_lag = X_no_lag.drop(columns=['DATE'])\n",
    "    X_no_lag = X_no_lag.loc[:, ~X_no_lag.columns.duplicated()]\n",
    "\n",
    "\n",
    "    y_no_lag = df.loc[:, df.columns.str.startswith('ROUTPUT')].iloc[:,0]\n",
    "\n",
    "\n",
    "    # Drop NA rows at the bottom of X_no_lag\n",
    "    #X_no_lag = X_no_lag.dropna(how='all', axis=0)\n",
    "    # Drop NA rows at the bottom of y_no_lag\n",
    "    #y_no_lag = y_no_lag.dropna(how='all')\n",
    "    X_lag = pd.DataFrame()\n",
    "    X_train = pd.DataFrame()\n",
    "    X_test = pd.DataFrame()\n",
    "\n",
    "    X_cols = list(set(X_no_lag.columns.tolist()))\n",
    "\n",
    "    for i in range(len(X_cols)):\n",
    "        c = X_cols[i]\n",
    "        try:\n",
    "            col_na = add_na (X_no_lag[c],c, horizon)\n",
    "            lagged_col = series_to_supervised(col_na, c, 12,horizon)\n",
    "            X_lag = pd.concat([X_lag, lagged_col], axis=1)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred for column: {c}, Error: {e}\")\n",
    "            \n",
    "    #X_lag = X_lag.dropna(how='all')\n",
    "\n",
    "    y_train = y_no_lag [horizon:,]\n",
    "    #X_train = X_lag.iloc[horizon:-horizon,]  \n",
    "    X_train = X_lag.iloc[horizon:horizon+len(y_train) ,]    \n",
    "    y_train = y_no_lag [horizon:,]\n",
    "    X_test = X_lag.iloc[-horizon:,]\n",
    "\n",
    "    rf_model = RandomForestRegressor(n_estimators= n_tree)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    if horizon < len(feature_importance)+1:\n",
    "    # If it exists, add rf_model.feature_importances_ to the existing one\n",
    "        feature_importance[horizon-1] += rf_model.feature_importances_\n",
    "    else:\n",
    "    # If it doesn't exist, append rf_model.feature_importances_ to the array\n",
    "        feature_importance.append([rf_model.feature_importances_])\n",
    "        feature_name.append(X_train.columns)\n",
    "    predictions = rf_model.predict(X_test) \n",
    "    return predictions\n",
    "\n",
    "#######################################input\n",
    "feature_importance_scores = []\n",
    "feature_names =[]\n",
    "n_rep = 5\n",
    "###########################################\n",
    "\n",
    "def rf (x1, x2, x3):\n",
    "    if not x3[1].isdigit():\n",
    "        horizon = int(x3[0])\n",
    "    else:\n",
    "        horizon = int(x3[0:2])\n",
    "    prediction_array = np.array([])\n",
    "    \n",
    "    for i in range(1, horizon+1):\n",
    "        for j in range (n_rep):\n",
    "            result = 0\n",
    "            \n",
    "            result = result + random_forest (x1, x2, i,feature_importance_scores, feature_names, n_tree = 25) [-1]\n",
    "        \n",
    "            #print(prediction_array)\n",
    "        prediction_array= np.append(prediction_array, result)\n",
    "        # Iterate over each element of the feature_importance list and divide by rep\n",
    "\n",
    "    return prediction_array/n_rep\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def get_feature ():\n",
    "    return feature_importance_scores\n",
    "\n",
    "def get_feature_names():\n",
    "    return feature_names\n",
    "\n",
    "\n",
    "\n",
    "#feature importance\n",
    "\n",
    "def get_unique_feature_impt():\n",
    "    dd = get_feature() \n",
    "    result = [[x_i / n_rep for x_i in x] for x in dd]\n",
    "    impt = []\n",
    "    for i in range (len(result)):\n",
    "        sub_array = result[i][0]\n",
    "        \n",
    "        # Iterate over the sub_array\n",
    "        sums = []\n",
    "        for i in range(0, len(sub_array), 12):\n",
    "            # Sum every 12 consecutive elements\n",
    "            subset_sum = sum(sub_array[i:i+12])\n",
    "            # Append the sum to the sums list\n",
    "            sums.append(subset_sum)\n",
    "        impt.append(sums)\n",
    "    return impt\n",
    "\n",
    "\n",
    "def get_unique_feature_names():\n",
    "    index_list = get_feature_names()\n",
    "    unique_modified_indices = []\n",
    "    for i in range(len(index_list)):\n",
    "        sub_list = index_list[i]\n",
    "\n",
    "\n",
    "        modified_indices = []\n",
    "        # Loop through the index list and extract the desired substring\n",
    "        for j in range(len(sub_list)):\n",
    "            index = sub_list[j]\n",
    "            # Extract the substring before '(' \n",
    "            start_index = index.find('(')\n",
    "            end_index = index.find(')')\n",
    "            if start_index != -1 and end_index != -1:\n",
    "                substring = index[0:start_index]\n",
    "                modified_indices.append(substring)\n",
    "\n",
    "        unique= list(set(modified_indices))\n",
    "        unique_modified_indices.append(unique)\n",
    "    # Display the modified indices\n",
    "    return unique_modified_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
